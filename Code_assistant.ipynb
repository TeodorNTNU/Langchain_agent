{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starcoder = 'starcoder2'\n",
    "llama = \"llama3\"\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "mistral_api_key = os.getenv(\"MISTRAL_API_KEY\") # Ensure this is set\n",
    "open_ai_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "# LCEL docs\n",
    "url = \"https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Sort the list based on the URLs and get the text\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code(prefix=\"This code demonstrates how to build a Retrieval-Augmented Generation (RAG) chain using LangChain and OpenAI's GPT model. The RAG chain combines retrieval of relevant documents with generative capabilities to produce informative responses.\", imports='from langchain.chains import RetrievalQA\\nfrom langchain.llms import OpenAI\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.document_loaders import TextLoader\\nfrom langchain.text_splitter import CharacterTextSplitter\\nimport os', code=\"# Load documents\\nloader = TextLoader('path/to/your/documents')\\ndocuments = loader.load()\\n\\n# Split documents into chunks\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nchunked_docs = text_splitter.split_documents(documents)\\n\\n# Create embeddings and vector store\\nembeddings = OpenAIEmbeddings()\\nvectordb = FAISS.from_documents(chunked_docs, embeddings)\\n\\n# Initialize the language model\\nllm = OpenAI(model_name='gpt-3.5-turbo')\\n\\n# Create the RAG chain\\nrag_chain = RetrievalQA(llm=llm, retriever=vectordb.as_retriever())\\n\\n# Example query\\nquery = 'What is the significance of RAG in machine learning?'\\nresponse = rag_chain.run(query)\\nprint(response)\", description=\"This code sets up a RAG chain using LangChain, allowing for retrieval of relevant documents and generation of responses using OpenAI's language model.\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "mistral_model = 'gpt-4o-mini'\n",
    "llm = ChatOpenAI(model=mistral_model, temperature=0)\n",
    "\n",
    "# Prompt \n",
    "code_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \n",
    "            \"\"\"You are a coding assistant. Ensure any code you provide can be executed with all required imports and variables \\n\n",
    "            defined. Structure your answer: 1) a prefix describing the code solution, 2) the imports, 3) the functioning code block.\n",
    "            \\n Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Data model\n",
    "class code(BaseModel):\n",
    "    \"\"\"Code output\"\"\"\n",
    "\n",
    "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "    imports: str = Field(description=\"Code block import statements\")\n",
    "    code: str = Field(description=\"Code block not including import statements\")\n",
    "    description = \"Schema for code solutions to questions about LCEL.\"\n",
    "\n",
    "\n",
    "code_gen_chain = code_gen_prompt | llm.with_structured_output(code)\n",
    "question = \"How do I build a RAG chain in LCEL?\"\n",
    "\n",
    "solution = code_gen_chain.invoke({\"context\":concatenated_content,\"messages\":[(\"user\",question)]})\n",
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix='This function calculates the Fibonacci sequence up to a specified number of terms.' imports='from typing import List' code='def fibonacci(n: int) -> List[int]:\\n    fib_sequence = []\\n    a, b = 0, 1\\n    for _ in range(n):\\n        fib_sequence.append(a)\\n        a, b = b, a + b\\n    return fib_sequence' description='This function generates the Fibonacci sequence up to the nth term.'\n"
     ]
    }
   ],
   "source": [
    "question = \"Write a function for fibonacci.\"\n",
    "messages = [(\"user\", question)]\n",
    "\n",
    "# Correctly invoking the method with a single dictionary containing both 'context' and 'messages'\n",
    "result = code_gen_chain.invoke({'context': 'Use your knowledge', 'messages': messages})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        error : Binary flag for control flow to indicate whether test error was tripped\n",
    "        messages : With user question, error messages, reasoning\n",
    "        generation : Code solution\n",
    "        iterations : Number of tries\n",
    "    \"\"\"\n",
    "\n",
    "    error: str\n",
    "    messages: List\n",
    "    generation: str\n",
    "    iterations: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "### Parameter\n",
    "\n",
    "# Max tries\n",
    "max_iterations = 3\n",
    "# Reflect\n",
    "# flag = 'reflect'\n",
    "flag = \"do not reflect\"\n",
    "\n",
    "### Nodes\n",
    "\n",
    "\n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate a code solution\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GENERATING CODE SOLUTION---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    error = state[\"error\"]\n",
    "\n",
    "    # We have been routed back to generation with an error\n",
    "    if error == \"yes\":\n",
    "        messages += [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:\",\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # Solution\n",
    "    code_solution = code_gen_chain.invoke(\n",
    "        {\"context\": concatenated_content, \"messages\": messages}\n",
    "    )\n",
    "    messages += [\n",
    "        (\n",
    "            \"assistant\",\n",
    "            f\"{code_solution.prefix} \\n Imports: {code_solution.imports} \\n Code: {code_solution.code}\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Increment\n",
    "    iterations = iterations + 1\n",
    "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
    "\n",
    "\n",
    "def code_check(state: GraphState):\n",
    "    \"\"\"\n",
    "    Check code\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, error\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECKING CODE---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    code_solution = state[\"generation\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    # Get solution components\n",
    "    imports = code_solution.imports\n",
    "    code = code_solution.code\n",
    "\n",
    "    # Check imports\n",
    "    try:\n",
    "        exec(imports)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE IMPORT CHECK: FAILED---\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "\n",
    "    # Check execution\n",
    "    try:\n",
    "        exec(imports + \"\\n\" + code)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE BLOCK CHECK: FAILED---\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "\n",
    "    # No errors\n",
    "    print(\"---NO CODE TEST FAILURES---\")\n",
    "    return {\n",
    "        \"generation\": code_solution,\n",
    "        \"messages\": messages,\n",
    "        \"iterations\": iterations,\n",
    "        \"error\": \"no\",\n",
    "    }\n",
    "\n",
    "\n",
    "def reflect(state: GraphState):\n",
    "    \"\"\"\n",
    "    Reflect on errors\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GENERATING CODE SOLUTION---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    code_solution = state[\"generation\"]\n",
    "\n",
    "    # Prompt reflection\n",
    "\n",
    "    # Add reflection\n",
    "    reflections = code_gen_chain.invoke(\n",
    "        {\"context\": concatenated_content, \"messages\": messages}\n",
    "    )\n",
    "    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n",
    "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
    "\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def decide_to_finish(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines whether to finish.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    error = state[\"error\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    if error == \"no\" or iterations == max_iterations:\n",
    "        print(\"---DECISION: FINISH---\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
    "        if flag == \"reflect\":\n",
    "            return \"reflect\"\n",
    "        else:\n",
    "            return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/4gHYSUNDX1BST0ZJTEUAAQEAAAHIAAAAAAQwAABtbnRyUkdCIFhZWiAH4AABAAEAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAACRyWFlaAAABFAAAABRnWFlaAAABKAAAABRiWFlaAAABPAAAABR3dHB0AAABUAAAABRyVFJDAAABZAAAAChnVFJDAAABZAAAAChiVFJDAAABZAAAAChjcHJ0AAABjAAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAAgAAAAcAHMAUgBHAEJYWVogAAAAAAAAb6IAADj1AAADkFhZWiAAAAAAAABimQAAt4UAABjaWFlaIAAAAAAAACSgAAAPhAAAts9YWVogAAAAAAAA9tYAAQAAAADTLXBhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABtbHVjAAAAAAAAAAEAAAAMZW5VUwAAACAAAAAcAEcAbwBvAGcAbABlACAASQBuAGMALgAgADIAMAAxADb/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wAARCAFCAHMDASIAAhEBAxEB/8QAHQABAAIDAQEBAQAAAAAAAAAAAAYHBAUIAwIJAf/EAFYQAAEDBAADAgYKCwwIBwAAAAECAwQABQYRBxIhEzEIFBYiQVEVF1VWYYGUldLTIzI2UlRxkZKT0dQJMzdCU3N0dqGytMEYJjR1sbPC4SRDREZXYoL/xAAbAQEBAAMBAQEAAAAAAAAAAAAAAQIDBAUGB//EADMRAAIAAwQIBAYCAwAAAAAAAAABAgMREiExUQQTFFJhkaHRBRVBcSMyM7HB8CKBQmLh/9oADAMBAAIRAxEAPwD9U6UrHuFwj2qE9LlOBqOynmWrRPT4AOpJ7gB1J6Cqk26IGRWDKvlugr5JNwix1/euvJSf7TWlbs0zK0CTelyIUJey3Z2XeTzT3F9aeql+tCVcg3rz9cxzo2E49CQEsWK2tJ1rzIjY3+Pp1rfZlw3Rur4d/wB9y3Ht5VWX3YgfKUfrp5VWX3YgfKUfrp5K2X3HgfJkfqp5K2X3HgfJkfqp8Hj0LcPKqy+7ED5Sj9dPKqy+7ED5Sj9dPJWy+48D5Mj9VPJWy+48D5Mj9VPg8eguHlVZfdiB8pR+unlVZfdiB8pR+unkrZfceB8mR+qnkrZfceB8mR+qnwePQXH9Tk9mWoJTdoKie4CSg/51sULS6gLQoLSRsKSdg1rFYnZFpKVWa3qSehBio0f7K168Ct0RZesvNj0rYVz24BDa9ehbWuRQPp6b9RB60pJeDa/r9/JLiS0rT2K9PS3n4FwZTFusYAuIR+9vIPc62T15T3EHqkgg76KVuK0xQuB0ZBSlKxAqMX7V1y6x2lelR2kOXN5B3pSmlIS0PiW5z/jbFSeoxOT4nxFtT6t9nMt8iKDrp2iVtuAb+FPaH/8AJrok/M3wf2KiT0pSucgqBxuOeEzczl4pGvK5V8iOOsvMsQpDjaXW2y4412yWy2XEpBJQFFXTWt1PK5rtHsxjfhCCPg1jy23We63iU9lMO724psyx2av/AB8WQrucW4lHmIUQvmJKEkboCZcI/CYx7ibg93yOUzLsbNpVLcmeMwZSWmo7L7raXA6tlKVqKGwpSEbUgkpIBFbyxeELgGR2DI7zBvxMLHYxm3RL8KQw/FYCFL7QsONpcKSlCiClJ3o63VN4pcs6wrgfnOFWDG79Bzq0S7pLiTVWwqiyGnrit0ORXVfY3XOxfKko3vmRojpqoXdcSutwm8UJNlx/iFPgXfhnMtkWblTEp6VMnIU4otJQ5tbZIdHKjlQFK7TkSfSBduc+FpiWOY1brzZ0zr/ElXeDbS+zbJoZLb7gCnWlhgh7lQFEBG+ZQCQeYgG4rFeouR2eJc4Xb+KSmw614zHcjucp7uZtxKVoPwKAPwVS/GLGLqeBGGexVkl3CRjtwsVzetMJnclTMV9lbqG2+hKwlJ0jv6aHXpVxYtkLeV2GLdWoVwtzcgKKY10iLiyUaUU+e0sBSd62NjuIPpoDa0pSgIxmmra5aL2jSXYcxuOtXXamH3ENLT09HMW1/jbFSeoxxBT4zZYsBOy7OuEVlAA30DyXFn4kIWfiqT1vi+lC3jfyu/NS+gpSlaCCtZkNlF8gJbS74vLZcTIiyOXm7J5P2qtbGx3gjY2lShsbrZ0rKGJwtRLEGls2RN3FxUCa2IF4bT9mgrVvmA71tE67Rs+hQHp0oJUCkRD/AEa+E/8A8bYr80MfRqdXixW+/sJZuEVuSlB5kFQ0ps61tKh1SdekEGtMcF7PpGyG+xW9aCBN7bQ/G6lavymt1JUd9bPVd/3EtxH1+DbwocUVK4b4spROyTaWCSfzasKFDYt0NiJFZbjRWG0tNMtJCUNoSNJSkDuAAAAqO+RMj31X79Mz9VTyJke+q/fpmfqqauXv9GKLMlFKi/kTI99V+/TM/VVUvgzXrIeLnDybe75lF1RMZvM6AkRFNIR2bLxQjoWz10Oppq5e/wBGKLM6CqFZLwT4f5leHrtfsKsN5ubwSHJk63NPOrCQEpBUpJJ0AAPgFZvkTI99V+/TM/VU8iZHvqv36Zn6qmrl7/RiizNArwbuFKwkK4cYuoIGkg2ljoNk6Hm+sn8tSSy4/ivCywuR7Tb7ZjFnDpdUzEZRHZLitDfKkAFR0B6zoCvLyIfOwrKL8oH0du0P7Q2DWVbcKtdtlomKQ/PnI6plXCQuQ4g920c5IR0+9A9PrpZlLGKvsu//AEXHlaoj99u7d8msLissIU3borw5XEpVrnecT/FWrWkp70p3vRWpKZHSlao47b4BilKVgQUpSgFKUoBSlKAVzv4C38DNz/rNdf8AEKroiud/AW/gZuf9Zrr/AIhVAdEUpSgFKUoBSlKAUpSgFKUoBSlKAUpSgFc7+At/Azc/6zXX/EKroiud/AW/gZuf9Zrr/iFUB0RSlKAUpSgFKUoBSlKAUpSgFKi14yucbg/AscFia9GITJkS31NMtqIBCByoUVq0QSOgAI676Vg+zuYfgFj+VvfV11Q6NMiVbl7tFoTelQj2dzD8Asfyt76uns7mH4BY/lb31dZbLHmuaFDmX90z4FHOeGkTPrXGLl4xnaZnINqcgLPnE+k9mshXqCVOE91cg+AHwbkcVfCCs09SVt2jFnG71KfSDrtG1gsN79anADr0pQv1V+p11k5LfLXMt0+0WCXBmMrjyI7sp4pdbWkpUkjs+4gkfHVYeDlwNuHg2Ytc7PY49qnruE5cx+bKkuB1Se5ts6a+1QnoPWVKPTm0Gyx5rmhQ6QpUI9ncw/ALH8re+rp7O5h+AWP5W99XTZY81zQoTelQj2dzD8Asfyt76uvpOT5ND+zTLRb5MZPVxMCW4XgPSUJU2Ao/Bsf5U2WZmuaFCa0rwgTmLnCjzIrgejPtpdbcT3KSRsH8le9cjTToyClKVAKUpQFf4x1l5EfSbs/1+JIre1osX/2rIv8Ae8j/AKa59448W8oteR8SoluzeLg5xOzR5trgOw2HXL2662tRO3QVFPOkMpDXULOzvoD6050i/cjKLE6fpXLt24o8SMnzO443YGskheTlut/jjtpt1slPvTJEcPHxjxt5oJSAQOVpA2Qvzk9BW5s+UcUM1zvHcXud4OAz3sR9lrnHhwo0lxEtMos7bU52iUpUCFEHn0Og0dqrTaMTomlcnRuP2eZdj/D6yWxucchusS4yrpcLDChuyCmHKMXbLcp1DKedQ5lE8xSNAJ67TtZ+d8Xo1hw22XB53Grvc80FmRcbhBiKel21UR13tHGWnHG0OBSSPMUAS2k6AUU0toHTlfKnEJWlBUkKVvlST1Ou/Vc68Xcpy/FpMfH8cznILtklutK7hKYgY/AkKWkrXyPS3F9m22g8vIEN8qyEEjdaNqbfeKnFfgfkzOSTMckXnDZVwcYgRoziGlHxRbqU9q2s8qysA72QG08vKSra0DqilcvZDxS4n5nl2bpwuLfGoeOXFy0Q2LZbrbIiyZDTaFLMpcmQh0BSl600E6Ro8yiSB0fjMy4XHG7VLu0IW26vxGnZcILCxHeUgFbfMCQeVRI2Cd6qp1Bl8LjvALN8DRA+AcxqVVFeFv3AWb+aP99VSqubSfrx+7+5XixSlK5yClKUBX+L/wC1ZF/veR/01RPHXhrmF74lqv2M2W/yJiILbUG5wLvbEx47ySojmZlsLcbSCQSWlHm9QI3V9TWpGI3S5OqgTJ1unSDKQ9BYL6mVlKQtC20gq6kFQUAR1IOtDm8fLON7mX75kl/VV7EUDnfygVVdgZNN3ogb3AheQu2zIbjkl5xzN3rVHhXy4YrKTGauK0I68yFtqGgor5VAJUArWx3CX27htbbbmsPKES579yi2QWJIkPBxK2A4lznWSOZThUkbUVdevTfWs3yzje5l++ZJf1VPLON7mX75kl/VVNRHusWXkQRfg1Y23jmPW2FdL5ap9gky5NuvkGUhucx4y6t19HN2ZQpCivXKpBGkp9PU70cHre9BxOPNvV7uj2OXU3iPMny0vPvvlDqNOqKOqNPq0lITrSQNAarMuHFjH7TdbZa5puMO5XMuJgw37XJQ9KKE8yw0gt7Xyp6nlB0OppeeLGP44qCm7KuFrVPkphxBMtklkyH1b5Wm+ZA51nR0kbJ0elNRHusWXkazMeCNpzDKX78bxfLLJmQkW64s2iYGG7hHSpRSh3zSoa7RYCm1IVpRG61j3g52UWDDrdBv+Q2mVibDsO2XaDKaRLTGcASplZLRQpHKhsdUbHIk73smceWcb3Mv3zJL+qp5Zxvcy/fMkv6qmoj3WLLyIdfPB/tVzyi5Xy3ZJk2LyLtyG6MWK4JjszloSEhxYKFFK+UAFTZQTrv3VoAaAH/GtB5Zxvcy/fMkv6qvpGTuzj2Nust3kSl9EJkwHorQPrW46lICR3nWzoHQJ0C1Ma9KEozbcLfuAs380f76qlVavF7J5OY9b7Z2vbKjMpQt3WudX8ZWvRs7Oq2ledOiUc2KJYNsPEUpStJBSlKAUpSgFKUoDnZ//Xvw5oqP3yDgeKKd339nOmucuvg2wmubP3V/PFi/4DikZ9TbkVh67vJQdHa1BtlXwEdk7+Wuk/BK/wBbL/xg4iK8/wAo8qdhxHf5SFCSGGFb+NY+KtPxu8A+xcfuJV8y/J8ruyFvxY8a1QLa20yiGG0EKDq1pcLoUs83mhvl2oedsEAaTwD/AAv0cacfbwzKpYTnNrZ+xPuK63SOkfvmz3upH247yPOG/O5evK5z8FjwQ8W4LWDHr7c8WtsfiXHhuR5lzjTH5SAVKIK2w4rlQtTYSFFCU65lpB5VHfRlAKUpQClKUApSlAKUpQClKUAqGcZ809rrhLmGTBfI7a7VJksn1uhs9mPjXyj46mdRnibh7fEHhzk+MuhPLd7bIggq7kqcbUlKviJB+KgId4KmF+QHg64BZ1J5Hxa25T6T3h1/b7gP4lOKHxVa9Uv4HGYOZr4NmDypBUJsKF7FSUL+3Q5GUpjSvhIbSfjq6KAqfhLAxXCeIHEPFLRf5tyv0u5HJrjbpaFcsLxoAJS0opAKCGx0Cla+Duq2KrvhZcVZPfM0vc3BPJG5NXZy0JuEhnkk3iLH0GZJJbSotnnUEAlQ6HR9FWJQClKUApSlAKUpQClK+VuIbG1qCR/9jqgPqleXjTP8s3+cKeNM/wAs3+cKtGD1qtuPfHix+DvhTGUZDbrtcbc7NRBKbQwh1xtS0LUFqC1oAR9jI3vvUka61YnjTP8ALN/nCtRluOWLOsauOP32PHuNouDKmJMZ4ghaT/aCDogjqCAQQRSjB+WuI/ugd24TDM7dgeMQ3Ldecql3uCu/KW4Y0Z7/AMjsWlJ0rYQrYcIT5ydK2FD9SG5l0awZqRcn4NvvnselUh47RFaklscx84khAWe4knXpNflrI8De6cMfDAwfE3Cq54lcrs3Mg3JaQQ/EZPbPNOa6BxKEFJ7t7CgACBX6J+ELPwq64bBw3Mn5pt2ZXKPY2kWtR7YvLV2iCSOqUbbHMrqNHR6GlGCUcJrNkmP8OrFb8uvbeSZGwxqbdGgAiQsqJBTpKdgJIG9AnWz161LaxYvikKKzHYU00wygNttpUNJSBoAfiFevjTP8s3+cKUYPWleXjTP8s3+cKeNM/wAs3+cKUYPWleaZDS1AJcQon0BQr0qAUpSgMW6TfY22S5fLzdgyt3l9fKkn/Kq8teJWq/W6Jcrzb4l4uUplDz0mcwl5W1AEpTzDzUDuCRoaHr2anOVfcxeP6G9/cNR7GvuctX9Ea/uCvS0duCW4oXR1MsEYXtfYt72rP8ga+jT2vsW97Vn+QNfRrRMcdMHlZgrF4978avCZPiSkx4j7jCJA72lPpQWkrHpSV7HqqG8I/Cdx/MokC33+5xIGUzLnNtyIseM+mOVtyXm2W+1UFIDqm20K5CvmPNsDRArbtEzffMlXmWf7X2Le9qz/ACBr6NPa+xb3tWf5A19Go/kPHvAsUyF6y3TIW4s5haG5CvF3lsRVr1ypefSgttE8wOlqSdEH016ZjxzwjAr0LRer2GbkGkvuR40V+Uphs/ard7JCuySfQV8opr5m++Yq8zee19i3vas/yBr6Nfw8PcVP/tqz/IGvo1CLVxyiQr1nyMmfjW+12O+RbRb1xo7rj8lT8ZhxCOzTzKccK3SAEJ7h3dCatcHYqqfMf+T5irzNB7X2Le9qz/IGvo09r7Fve1Z/kDX0a0ubcbsK4d3Zu1329eL3FTPjBixor8pxtreu0cSyhZbRsHzl6HQ9eleWX8ecEwS4twb3fkxZSo6Za0Nxnn+wZV9q68W0KDKD185wpHQ1NfM33zFXmb/2vsW97Vn+QNfRp7X2Le9qz/IGvo1qMy404XgL1uZvV8baeuLRkRWYrLspbjI1t3lZSshvr9uQE/DUSwHwibVK4OYfl2ZTI1tuF/aWpuJbYr7ynlJUoHsmUdo4oBIBJG9b6kbFNomYW3zFXmWGeH2La+5u0D4RBaB/u1t8FluIfvFoW64+zbnm0x1uqKlpaW2lQQVHqeU8wBOzrWyTWvxHMbLnljZvFguDVytzqlIS81saUk6UlSSAUqBBBSoAg94rJwr7qss/nY3/ACRUjjimSo7TrRflFrVOpM6UpXkmJq8q+5i8f0N7+4aj2Nfc5av6I1/cFSTI2VyMeujTaSpxcV1KUj0koIFRrF1pcxq0qSdpVEZIPrHIK9CT9F+/4L6FIeD3fLlwwxez8N71huSi9QZj8d26xrapy3SUrfWsS/GQeTlUFBSgTzgkjlqNQsLvzfg443bzYrim6x87ROVF8TcD7bQvy3O2KNcwT2R5+bWuU73qup6Us+hDkKBw6j2m+Zri+a4zxEvHs3f5klh/H5872Knw5TnMC6GnkstqSFFK0uAbCenNuptjU6bwI4g8QGJuG5Jfod9msT7Vc7FAVO7VlEVpkRnVA/Y1NlsgFwhJCt7FdD0pZoDke78Nspg8W864oQYF2nLsGRRp8LHXIn2K4xlW9lqW4wCna3wkkIUknSmVJHVZrrOLITLjMvoS4lDqAsJdQpCwCN6UlQBSfWCARXrUDuHAXhtdp8mdNwPHZc2S6p5+Q9bGVrdcUSVKUop2SSSST66JUwBXzF2uPB7jJxFuVyxHIcgg5Q5DmW65WC3Km7DUZLKozvL1aKVJJSVaSQsnYO61V1m3Xh/mPFZ+RhGQZEnNWI0m1rt1vMlKyIYYMSQpOwzyrSTtZCeVZO+8V0NZ7NAx61xrbbIbFvt8ZAbYixmw220kdwSkdAPxVmUsg5X4W2LIPB6yqGcjxq95M3cMTs9rZuFihKneJPxUOJeir5eqEKUtKgs6SddT6obiPDu+Y3YuE+QZBj2aKtEbHJVnmwsbclxbnbZC5XbIW40wpDqm1pTykDeiEEjoK7apUsAgXBbGrNj+IOv2a03yzN3aa9cJEfI33XZy3lEILjhdWtQKktpVoneiNgEkVLcK+6rLP52N/wAkVsKwcKbPlJlbmvML8dG9fxgwkkfkUn8tbMJUft+UZLBkxpSleYYionK4fJ7dxdsvdysbK1FZiwwwtkKPUlKXWl8uz10kgbJOutSylbIJkUv5WWtCG+QFw9+d7/QQv2enkBcPfne/0EL9nqZUrdtMzhyXYVIb5AXD353v9BC/Z6eQFw9+d7/QQv2eplSm0zOHJdhUhvkBcPfne/0EL9nqK8N8L4gTLA8vN8mkQbwJj6W2rU3DU0Y4X9iUeZhR5inW+vf6BVuVWXg92qyWfBZbFgyt7MYRusxxVwfUVKQ6XSVs9fQg+bTaZnDkuwqbvyAuHvzvf6CF+z08gLh7873+ghfs9TKlNpmcOS7CpDfIC4e/O9/oIX7PTyAuHvzvf6CF+z1MqU2mZw5LsKkOTgE7fn5je1p9Keyhp38Yj7qSWizxbHBREht8jSSVEqJUpaj1UpSj1UonqSeprNpWuOdHMVInd7JfYVFKUrSQUpSgFKUoBSvlauVJPqG60HlSr8HH5/8A2oCQ1WXg93WyXjBZb9gxR7DoQusxtVvfSUqW6HSFvdfQs+dUs8qVfg4/P/7VqcZvN4ttvW1eZzV6ll5a0yG44jBLZO0o5QVb5R05t9fUKAm9KicjiBCiXGHb33YrM+YFqjRXJCUuvhABWUJPVXKFDeu7Y331v7VcDcWFuFAb5Vcugd+igM2lKUApSlAKUpQClKUApSlAfDv70v8AEa5y8IDiJecEsdgiY9HeevWQXZu1sOR2WnXGQUOOLWhDq0NqXytkJC1AbVs71o9HOAltQHeQaqriFwijcTLAm1XaPKaSzIbmRZcJ/sZESQ2dtvNLH2q09dH4SCCDQHPd84hcWsPwDLpc5u4xUxXLWbVecggwESCt2a2y+y4zFdW2tHIoaUAhXnKHQgKrbZZxXyjgndc3iXi7nMmYGLoyCC5KiNRltvl9TBaV2KUgtlXIrZHMBvqe+rOkeDubnhlzxy73/Jb4zcJMaS7MuM1px9BYdbdQlGmwhCSptOwEddn09a+Rwyxvi1c7tkhbl3SPPtLuMSorySyypkOqWvzVJSsL5iQFA61ojr1oCr2sey2x8feFC8ry0ZNKkQbwrs0W9qM3Fc7FjnS2UAFSDsAc+z5u99dDrDGP9jd/nP8AIVTGKeDQjF8msN7cyHKL7JsbD8aA1eJzTzbTTqUpUnQbSToITok76dSdCrtsEV2JFcS8goUV7APq0KA2dKUoBSlKAUpSgFKUoBSlKAUpSgFQvhP5beTL/l94l7N+PSOy8Q12fi3Oex3r+Ny63VE+G54SHEnwaxj13xe1Y/csZuPNFfdukV9x1iUNqA5kPIHKtHcNE7bXs9QK5k8ELwyeKNwz7GOHUG2Wi8wbzfFSJkiU3IcltR3HO0klCy+EgNthZSCk65f41AfqLSlKAUpSgFKUoBSlKAUpSgFYV0vdusbIduM+Lb2j0C5TyW0n41EVX3E3iU/bJK7JZHA3ORoypnKFCOCNhCQehWQQeoISCO8kaqAwWXJK5T4MqYv7eVJUXXV/jWrZP5a+h0PwiPSIFMmRWU8M32FyxOhzxOw5JIOWWMEdCDcmfpV/PbQw3322L5yZ+lXP/In70fkpyJ+9H5K9PyOTvvoKosDj3FwDjfwmyHD5eW2Btc5gmK+q4s/YJKPOaX9tvQUBvXekqHprlv8Ac4uGVk4aHI81zK52u0X51arXb4k6Y0260ykguu8pV3LUEhKvUhRBIV1uTkT96PyU5E/ej8lPI5O++gqjoD20MN99ti+cmfpU9tDDffbYvnJn6Vc/8ifvR+SnIn70fkp5HJ330FUdGW3N8cvL6WIF/tc55X2rcaa24o/iAUTW7rlORBjS0FD8dp5BGilxAUD+WpPhue3HCpDaFuv3Cyb05DWouLZH3zJPXp95vRHdo9/JpHgjhhcUmKryfcXM6FpXhCmsXGGxLiupfjPtpdadQdpWlQ2CD6iDXvXy7VLmBSlKgFYt1uDdptcyc6NtRmVvLA9SUkn/AIVlVgX+2m82K428K5DLjOMBR9HMkp3/AG1nBS0rWAOYYLj0iOJMpfaS5JMh9z75xZ5lH8pOvUNV71jWtxblujlxCm3QgJcbUNFCx0Uk/CCCPirAv2ZWDFlsovV8ttoW8CWkz5bbBWB3lPMRvWx3euv1SJww43IxeJuK02WZZBw61pmzUvvF11MePGitlx6Q6r7VttPpUdH1AAEkgAmtZ7bmC635aY9r/erH06hvFCNaeMNptgxmdYsvl2O4NXF2zia063Ka5VoU2ogqCSQs8qlDWxXPMnJQPVtNg3cjjfZrdaLvMuVtvFqk2oxjKtsuKnxkNvuhptxKUqUlaeYn7VRPmka3oHPtvFi1SX7wxc4k/HH7VEFwfbu7SUExjzfZk8ilAp2hQI2FA9CKgd14cPXTh5fI9m4cwsRukmTBDceO9G7V9puS04srU2eQABKiBzHevWdVseKPC67Z5k2RJjpTHhXDFfY1mYtaeUSRJLiUKTvm1rWzrWifT0rl1mkJVSr/AFjj7ZIHpA4tzcm4k4bb4VtvFms9wjTn3BdIaGhMSlDZaWg7Uoa2To8p84bHdVt1TwkZRdM4xHIslxtrFrXYok4T5j9zjrZSpxtsBQ0rYRtB6nr6wPTNfbdwX364786sfTrdJmUtayL1urd6L0fEEspUXjcVMKmyWo8fL7C++6sNttNXNhSlqJ0EgBWySemqlFdcMUMXyupC2OA90W7Y7palq5k2+WSyPvWnEhYT8Sy5r4ND0VZ1VVwFgrEK/wBxUCG5EtLDZP8AGDaBsj4OZah+NJq1a/PPElCtLmWc+tL+psYpSleYQUpSgKX4ocPn7VPk361sLkQJKy7NjtJ2thw/bOpA70K71AdQdq6gq5a6R4tcGkPJ7KS2obQ4NLBHwGurail84W4vkMpyVKtSG5ThJckRHVxnFne9qU2pJUfx7r6fQ/GNVApekJtL1WIuZz94lH/B2vzBX22w0ySW20IJ7+VIFXQeBuLk71cfnF76VPaMxf1XL5xe+lXp+daJlFyXcUWZTVKuX2jMX9Vy+cXvpU9ozF/VcvnF76VXzrRf9uS7iizKZUkLSUqAUk94I6GvLxGMf/TtfmCrr9ozF/VcvnF76VPaMxf1XL5xe+lU860TKLku4osylRDYSQQw2COoIQK2eN2CdmdyMG1a0hXLJmkczUUenm9a/UjvOxvQ2RbsbgliTKwpyFJma/iypz7iD+NBXyn4wamkKDGtsVuLDjtRYzY0hlhAQhI9QA6CuWf43LUNNHhdc36feookY9hskTG7PEtkFBRFjNhtAUdqPrUT6STsk+kkms+lK+RiicTcUWLApSlYgUpSgFKUoBSlKAUpSgFKUoBSlKAUpSgFKUoD/9k=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from IPython.display import Image, display\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"generate\", generate)  # generation solution\n",
    "workflow.add_node(\"check_code\", code_check)  # check code\n",
    "#workflow.add_node(\"reflect\", reflect)  # reflect\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"generate\")\n",
    "workflow.add_edge(\"generate\", \"check_code\")\n",
    "workflow.add_conditional_edges(\"check_code\",\n",
    "        decide_to_finish,\n",
    "    {\n",
    "        \"end\": END,\n",
    "        #\"reflect\": \"reflect\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "#workflow.add_edge(\"reflect\", \"generate\")\n",
    "my_graph = workflow.compile()\n",
    "display(Image(my_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "Processed input: Hello, this is a test string.\n",
      "---NO CODE TEST FAILURES---\n",
      "---DECISION: FINISH---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'error': 'no',\n",
       " 'messages': [('user',\n",
       "   'How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?'),\n",
       "  ('assistant',\n",
       "   \"This code demonstrates how to pass a string to a runnable function and use it to construct the input needed for a prompt. The example includes a simple function that takes a string and processes it to generate a prompt input. \\n Imports: import sys \\n Code: def process_input(input_string):\\n    # Constructing the prompt input from the string\\n    prompt_input = f'Processed input: {input_string}'\\n    return prompt_input\\n\\nif __name__ == '__main__':\\n    # Example string to pass\\n    input_string = 'Hello, this is a test string.'\\n    result = process_input(input_string)\\n    print(result)\")],\n",
       " 'generation': code(prefix='This code demonstrates how to pass a string to a runnable function and use it to construct the input needed for a prompt. The example includes a simple function that takes a string and processes it to generate a prompt input.', imports='import sys', code=\"def process_input(input_string):\\n    # Constructing the prompt input from the string\\n    prompt_input = f'Processed input: {input_string}'\\n    return prompt_input\\n\\nif __name__ == '__main__':\\n    # Example string to pass\\n    input_string = 'Hello, this is a test string.'\\n    result = process_input(input_string)\\n    print(result)\", description='This code takes a string, processes it, and constructs a prompt input.'),\n",
       " 'iterations': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\n",
    "my_graph.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
